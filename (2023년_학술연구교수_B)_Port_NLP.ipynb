{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1j5v1nepuSSEOJSS26wwt9xx1rs1lbxyQ",
      "authorship_tag": "ABX9TyO+tE/oZRZpBwo2OBL8jOPV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamiroquai1225/NLP/blob/main/(2023%EB%85%84_%ED%95%99%EC%88%A0%EC%97%B0%EA%B5%AC%EA%B5%90%EC%88%98_B)_Port_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GwX4S4Hnh-cf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZYEO_vJhhRcC"
      },
      "outputs": [],
      "source": [
        "data = '/content/drive/MyDrive/한국연구재단/wos.xlsx'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_excel(data)\n",
        "data = data_df\n",
        "print(data)"
      ],
      "metadata": {
        "id": "VoXexBDSh6UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = data"
      ],
      "metadata": {
        "id": "j09hFyDTh7AC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_df[data_df['abs'].notnull()]"
      ],
      "metadata": {
        "id": "EKI3ylAFiIB6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrhHUVmAiTA7",
        "outputId": "26153fa6-601b-438e-cab3-66c355b621c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob # 경로와 이름을 지정하여 파일을 다루는 파일처리 작업을 위한 모듈\n",
        "import re #메타문자를 이용하여 특정 규칙을 작성하는 정규식을 사용하기 위한 모듈\n",
        "from functools import reduce # 2차원 리스트를 1차원 리스트로 차원을 줄이기 위한 모듈\n",
        "\n",
        "#자연어 처리 패키지(from nltk.tokenize) 중에서 단어 토큰화 작업을 위한 모듈\n",
        "from nltk.tokenize import word_tokenize\n",
        "# 자연어 처리 패키지(from nltk.corpus) 중에서 불용어 정보를 제공하는 모듈\n",
        "from nltk.corpus import stopwords\n",
        "# 자연어 처리 패키지(from nltk.stem) 중에서 단어 형태의 일반화를 위해 표제어 추출을 제공하는 모듈\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# 데이터 집합에서 개수를 자동으로 계산하기 위한 모듈\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "#워드클라우드를 그리기 위해 사용할 워드클라우드용 불용어 모듈과 워크클라우드 모듈"
      ],
      "metadata": {
        "id": "K4VBjWOeiTfF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = df['abs']\n",
        "documents = df['abs']\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0sUoe49iThQ",
        "outputId": "7ddb128f-69c9-4756-f311-bd7611e25398"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1017"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPWNUI4xiTjk",
        "outputId": "ba13b00c-2efb-4000-a176-5879ebf71248"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-8524ed06c28b>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-cpXzkxiTl8",
        "outputId": "732ee2f4-4b1b-44d1-c0ed-1df3dac20286"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
        "stop_words.extend(['result', 'use', 'also', 'analyze', 'use', 'study', 'research','analysis','paper','purpose','using','first','toward','directly',\n",
        "                   'among','purpose','results'])\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "# 불용어를 제거합니다."
      ],
      "metadata": {
        "id": "nsByIxkbiToD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://blog.naver.com/jin_come_up/222498163391\n",
        "text3 = list(reduce(lambda x, y: x+y, tokenized_doc))"
      ],
      "metadata": {
        "id": "1rXZ4qJgicfZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = Counter(text3)"
      ],
      "metadata": {
        "id": "J5m7Kacyicl9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_count = dict()\n",
        "\n",
        "for tag, counts in count.most_common(30):\n",
        "    if(len(str(tag))>1):\n",
        "        word_count[tag] = counts\n",
        "        print(\"%s : %d\" % (tag, counts))"
      ],
      "metadata": {
        "id": "sknTHoBFicpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#검색어로 사용한 'big'과 'data' 항목 제거 하기\n",
        "del word_count['elsevier']\n",
        "del word_count['rights']\n",
        "del word_count['reserved']\n",
        "del word_count['used']"
      ],
      "metadata": {
        "id": "W0gzXJ3dicr8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmPTPCOdicuX",
        "outputId": "9694e534-eed6-4b21-97ae-50f72da8f260"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "4ufCuNr0icwY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []for string in sent_text:\n",
        "  tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "  normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "metadata": {
        "id": "wI0O3mc9icym"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('총 샘플의 개수 : {}'.format(len(tokenized_doc)))\n",
        "result = tokenized_doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK8bthREic8G",
        "outputId": "6b29aca2-93e3-45ec-fe50-7c439d4322ce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플의 개수 : 1017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for line in result[:3]:\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZAO6AgIiTqZ",
        "outputId": "a1589be1-fcf3-49c2-bc17-ce0bd215f8ba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ireland', 'trade', 'dependent', 'country', 'european', 'union', 'peripheral', 'island', 'economy', 'effective', 'seaport', 'system', 'essential', 'link', 'international', 'trade', 'facilitate', 'export', 'development', 'growth', 'maritime', 'trade', 'dispersed', 'benefit', 'ireland', 'ports', 'although', 'recently', 'traffic', 'polarised', 'strongly', 'ports', 'cork', 'dublin', 'rosslare', 'shannon', 'estuary', 'waterford', 'four', 'processes', 'played', 'roles', 'changing', 'comparative', 'advantages', 'irish', 'ports', 'stimulating', 'trade', 'deflection', 'trade', 'northern', 'ireland', 'ports', 'europeanisation', 'globalisation', 'industrial', 'policy', 'economic', 'restructuring', 'peripherality', 'transport', 'policy', 'national', 'development', 'plans', 'used', 'structural', 'funds', 'reduce', 'costs', 'peripherality', 'focused', 'investment', 'four', 'strategic', 'national', 'ports', 'service', 'corridors', 'trade', 'across', 'irish', 'context', 'port', 'management', 'created', 'harbours']\n",
            "['major', 'changes', 'ports', 'marine', 'shipping', 'united', 'states', 'since', 'resulted', 'containerization', 'intermodalism', 'increasing', 'scale', 'reduced', 'rail', 'road', 'freight', 'shipping', 'casts', 'advances', 'freight', 'logistics', 'information', 'technology', 'integration', 'world', 'markets', 'consequently', 'ports', 'face', 'great', 'uncertainty', 'benefits', 'increasingly', 'dispersed', 'locally', 'less', 'significant', 'adverse', 'impacts', 'remain', 'localized', 'fragmented', 'ineffective', 'public', 'responses', 'remain', 'rooted', 'tradition', 'economics', 'achieved', 'vast', 'scale', 'long', 'lived', 'inflexible', 'investments', 'spurred', 'excessive', 'subsidized', 'competition', 'propose', 'future', 'oriented', 'agenda', 'improve', 'public', 'policy', 'ports', 'reviews', 'nations', 'approaches', 'comparative', 'scholarly', 'case', 'studies', 'economic', 'roles', 'maritime', 'ports', 'whether', 'institutional', 'funding', 'arrangements', 'match', 'roles', 'prospective', 'benefit', 'cost', 'analyses', 'publicly', 'funded', 'port', 'expansion', 'plans', 'identify', 'variables', 'affect', 'ports', 'amenable', 'policy', 'influence']\n",
            "['main', 'present', 'pricing', 'mechanism', 'appropriate', 'allocating', 'common', 'maritime', 'infrastructure', 'cost', 'would', 'allow', 'fair', 'efficient', 'allocation', 'solution', 'would', 'take', 'account', 'demand', 'characteristics', 'assuring', 'realistic', 'interpretation', 'market', 'behaviour', 'proposed', 'solution', 'concept', 'ratio', 'equilibrium', 'main', 'qualities', 'consist', 'guaranteeing', 'balanced', 'budget', 'strategic', 'point', 'view', 'stable', 'solution', 'words', 'port', 'user', 'discouraged', 'infrastructure', 'assignment', 'cost', 'share', 'high', 'latter', 'feature', 'adds', 'competitive', 'element', 'calculations', 'considered', 'innovative', 'feature', 'numeric', 'example', 'presented', 'elsevier', 'science', 'rights', 'reserved']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences=tokenized_doc,vector_size=100, window=3, min_count=5, workers=10, sg=1) # window = 5\n",
        "# 만약 TypeError: __init__() got an unexpected keyword argument 'size' 라는 에러 발생 시에는\n",
        "# size 대신 vector_size로 바꿔서"
      ],
      "metadata": {
        "id": "RQx-VZTxiTsv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_result = model.wv.most_similar(\"model\")\n",
        "print(model_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wNNKvNiiTvC",
        "outputId": "d99e3ec7-093c-44ab-d417-8eaf12a24c49"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('linear', 0.9688355922698975), ('programming', 0.958905816078186), ('integer', 0.958564817905426), ('mixed', 0.9582169055938721), ('mathematical', 0.9511022567749023), ('simulation', 0.9475018382072449), ('models', 0.9437374472618103), ('developed', 0.9436261653900146), ('methods', 0.9335346817970276), ('optimization', 0.9334251284599304)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter Text"
      ],
      "metadata": {
        "id": "Ju1HC9GXi7mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/content/drive/MyDrive/한국연구재단/wos.xlsx'"
      ],
      "metadata": {
        "id": "Gkc9yuS0nQkZ"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_excel(data)"
      ],
      "metadata": {
        "id": "itT-oW4knScp"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scattertext"
      ],
      "metadata": {
        "id": "0o37kRxAi2NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scattertext as st\n",
        "import spacy\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "fTGGvyOOi2Po"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# from replacers import RegexpReplacer\n",
        "# 불용어 리스트를 불러옵니다.\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "gQDEE6p1i2Re"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scattertext.CorpusFromParsedDocuments import CorpusFromParsedDocuments\n",
        "from scattertext import SampleCorpora, whitespace_nlp_with_sentences, produce_scattertext_explorer"
      ],
      "metadata": {
        "id": "Cy6cGbzylqN3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convention_df = data_df\n",
        "convention_df.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvMzhJ7UjCm2",
        "outputId": "9b893e0a-1c58-465c-f24f-23386a44af86"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author                                                    Brunt, B\n",
              "title                                     Ireland's seaport system\n",
              "source           TIJDSCHRIFT VOOR ECONOMISCHE EN SOCIALE GEOGRAFIE\n",
              "type                                                       Article\n",
              "keyword          Ireland; seaports; peripherality; traffic stru...\n",
              "Keywords Plus                                               EUROPE\n",
              "abs              Ireland is the most trade dependent country in...\n",
              "cited                                                            7\n",
              "year                                                          2000\n",
              "time                                                     top 00-09\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgt9OH4hAidS",
        "outputId": "f5965970-29b0-4bf4-c211-e0f793b20f7c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# from replacers import RegexpReplacer\n",
        "# 불용어 리스트를 불러옵니다.\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "PXPaMRcbAifa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "paragraph = convention_df.lower()\n",
        "print(paragraph)"
      ],
      "metadata": {
        "id": "Gp7LopjCAihw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "data_df['clean_doc'] = data_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "data_df['clean_doc'] = data_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "data_df['clean_doc'] = data_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IptIlePJbPz",
        "outputId": "8161cef1-2725-445f-defc-8ab9e5ecccef"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-117-8cf5a5e5ea52>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data_df['clean_doc'] = data_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df"
      ],
      "metadata": {
        "id": "aeYj9uyjKN24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_excel(data)"
      ],
      "metadata": {
        "id": "YAGteh0zOXED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = data1, data_df"
      ],
      "metadata": {
        "id": "nj08B3H3OS9r"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1"
      ],
      "metadata": {
        "id": "Gy58j7NgOyVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convention_df = data1\n",
        "convention_df"
      ],
      "metadata": {
        "id": "tS1_dmzeO965"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
        "stop_words.extend(['result', 'use', 'also', 'analyze', 'use', 'study', 'research','analysis','paper','purpose','using','first','toward','directly',\n",
        "                   'among','purpose','results'])\n",
        "convention_df['clean_doc'] = convention_df['clean_doc'].apply(lambda x: x.split()) # 토큰화"
      ],
      "metadata": {
        "id": "Uxptt5inI858"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
        "stop_words.extend(['result', 'use', 'also', 'analyze', 'use', 'study', 'research','analysis','paper','purpose','using','first','toward','directly',\n",
        "                   'among','purpose','results'])\n",
        "tokenized_doc = data_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "# 불용어를 제거합니다."
      ],
      "metadata": {
        "id": "GHydX2vDJwXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "corpus = st.CorpusFromPandas(convention_df,\n",
        "                             category_col='time',\n",
        "                             text_col='clean_doc',\n",
        "                             nlp=nlp).build()"
      ],
      "metadata": {
        "id": "ixDXe1y2jCpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(corpus.get_scaled_f_scores_vs_background().index[:10]))"
      ],
      "metadata": {
        "id": "9Vf5nZlLjCsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46228541-5b3a-42ed-cc76-9eb017ef4f2f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['seaports', 'hinterland', 'seaport', 'transhipment', 'intermodal', 'transshipment', 'envelopment', 'logistics', 'repositioning', 'elsevier']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "term_freq_df = corpus.get_term_freq_df()\n",
        "term_freq_df['Top 00-09 Score'] = corpus.get_scaled_f_scores('top 00-09')\n",
        "pprint(list(term_freq_df.sort_values(by='Top 00-09 Score', ascending=False).index[:10]))"
      ],
      "metadata": {
        "id": "DzBw3suqjCus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1191c027-39cf-4260-fbbe-bdd1a2ba19ff"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train',\n",
            " 'comprehensive',\n",
            " 'product',\n",
            " 'productivity',\n",
            " 'warehousing',\n",
            " 'oil',\n",
            " 'logistics system',\n",
            " 'the problem',\n",
            " 'chinese',\n",
            " 'ports hinterland']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "term_freq_df = corpus.get_term_freq_df()\n",
        "term_freq_df['Top 10-19 Score'] = corpus.get_scaled_f_scores('top 10-19')\n",
        "pprint(list(term_freq_df.sort_values(by='Top 10-19 Score', ascending=False).index[:10]))"
      ],
      "metadata": {
        "id": "7Ou3mUhAjCzK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84628e3d-9675-47eb-a3f4-46cc51897228"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['inland',\n",
            " 'seaports',\n",
            " 'role',\n",
            " 'road',\n",
            " 'research',\n",
            " 'of a',\n",
            " 'intermodal',\n",
            " 'transport',\n",
            " 'were',\n",
            " 'maritime']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html = st.produce_scattertext_explorer(corpus,\n",
        "                                       category='top 00-09',\n",
        "                                       category_name='00-09',\n",
        "                                       not_category_name='10-19',\n",
        "                                       width_in_pixels=1000,\n",
        "                                       metadata=convention_df['time'])\n",
        "\n",
        "open(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))"
      ],
      "metadata": {
        "id": "h3HmC5GOjC1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b88b16-4d55-4904-935c-7b304fa205e4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2338466"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FywOh0_6Egpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9v1kDImQEgr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZwDt6Y3Egvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OtjMDz3hEgyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSmKPeuPEg2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pz5KolGhEg3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}