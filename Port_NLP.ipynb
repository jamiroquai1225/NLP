{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1j5v1nepuSSEOJSS26wwt9xx1rs1lbxyQ",
      "authorship_tag": "ABX9TyMGD/A+yAD15m2NRd+YJUb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamiroquai1225/NLP/blob/main/Port_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GwX4S4Hnh-cf"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ZYEO_vJhhRcC"
      },
      "outputs": [],
      "source": [
        "data = '/content/drive/MyDrive/한국연구재단/wos.xlsx'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_excel(data)\n",
        "data = data_df\n",
        "print(data)"
      ],
      "metadata": {
        "id": "VoXexBDSh6UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = data"
      ],
      "metadata": {
        "id": "j09hFyDTh7AC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_df[data_df['abs'].notnull()]"
      ],
      "metadata": {
        "id": "EKI3ylAFiIB6"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "MrhHUVmAiTA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob # 경로와 이름을 지정하여 파일을 다루는 파일처리 작업을 위한 모듈\n",
        "import re #메타문자를 이용하여 특정 규칙을 작성하는 정규식을 사용하기 위한 모듈\n",
        "from functools import reduce # 2차원 리스트를 1차원 리스트로 차원을 줄이기 위한 모듈\n",
        "\n",
        "#자연어 처리 패키지(from nltk.tokenize) 중에서 단어 토큰화 작업을 위한 모듈\n",
        "from nltk.tokenize import word_tokenize\n",
        "# 자연어 처리 패키지(from nltk.corpus) 중에서 불용어 정보를 제공하는 모듈\n",
        "from nltk.corpus import stopwords\n",
        "# 자연어 처리 패키지(from nltk.stem) 중에서 단어 형태의 일반화를 위해 표제어 추출을 제공하는 모듈\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# 데이터 집합에서 개수를 자동으로 계산하기 위한 모듈\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "#워드클라우드를 그리기 위해 사용할 워드클라우드용 불용어 모듈과 워크클라우드 모듈"
      ],
      "metadata": {
        "id": "K4VBjWOeiTfF"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "dataset = df['abs']\n",
        "documents = df['abs']\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0sUoe49iThQ",
        "outputId": "fd481d9c-ee0b-42c1-9304-aa176b4e4d31"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1017"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPWNUI4xiTjk",
        "outputId": "846ef9e0-dd85-463d-9cf6-56f79340205f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-8524ed06c28b>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "O-cpXzkxiTl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
        "stop_words.extend(['result', 'use', 'also', 'analyze', 'use', 'study', 'research','analysis','paper','purpose','using','first','toward','directly',\n",
        "                   'among','purpose','results'])\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "# 불용어를 제거합니다."
      ],
      "metadata": {
        "id": "nsByIxkbiToD"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://blog.naver.com/jin_come_up/222498163391\n",
        "text3 = list(reduce(lambda x, y: x+y, tokenized_doc))"
      ],
      "metadata": {
        "id": "1rXZ4qJgicfZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = Counter(text3)"
      ],
      "metadata": {
        "id": "J5m7Kacyicl9"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_count = dict()\n",
        "\n",
        "for tag, counts in count.most_common(50):\n",
        "    if(len(str(tag))>1):\n",
        "        word_count[tag] = counts\n",
        "        print(\"%s : %d\" % (tag, counts))"
      ],
      "metadata": {
        "id": "sknTHoBFicpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = word_count\n",
        "df1.to_excel()"
      ],
      "metadata": {
        "id": "yI4fjTXzsFcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#검색어로 사용한 'big'과 'data' 항목 제거 하기\n",
        "del word_count['elsevier']\n",
        "del word_count['rights']\n",
        "del word_count['reserved']\n",
        "del word_count['used']"
      ],
      "metadata": {
        "id": "W0gzXJ3dicr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "rmPTPCOdicuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "4ufCuNr0icwY"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "  tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "  normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "metadata": {
        "id": "wI0O3mc9icym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('총 샘플의 개수 : {}'.format(len(tokenized_doc)))\n",
        "result = tokenized_doc"
      ],
      "metadata": {
        "id": "ZK8bthREic8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in result[:3]:\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "kZAO6AgIiTqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences=tokenized_doc,vector_size=100, window=3, min_count=5, workers=10, sg=1) # window = 5\n",
        "# 만약 TypeError: __init__() got an unexpected keyword argument 'size' 라는 에러 발생 시에는\n",
        "# size 대신 vector_size로 바꿔서"
      ],
      "metadata": {
        "id": "RQx-VZTxiTsv"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences=tokenized_doc,vector_size=100, window=3, min_count=5, workers=10, sg=1) # window = 5\n",
        "# 만약 TypeError: __init__() got an unexpected keyword argument 'size' 라는 에러 발생 시에는\n",
        "# size 대신 vector_size로 바꿔서"
      ],
      "metadata": {
        "id": "fZFSGutDw6yc"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_result = model.wv.most_similar(\"maritime\")\n",
        "print(model_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wNNKvNiiTvC",
        "outputId": "465f0b8c-419a-4b2a-8b00-49e5e85aafcf"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('networks', 0.9722511768341064), ('hinterland', 0.970900297164917), ('road', 0.969731867313385), ('distribution', 0.9674057960510254), ('land', 0.9663898348808289), ('freight', 0.9661656618118286), ('chain', 0.9624977707862854), ('become', 0.960765540599823), ('inland', 0.957770824432373), ('intermodal', 0.9573519825935364)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_result"
      ],
      "metadata": {
        "id": "EREfaaTLwd3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vehhkdYNISX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter Text"
      ],
      "metadata": {
        "id": "Ju1HC9GXi7mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/content/drive/MyDrive/한국연구재단/wos.xlsx'"
      ],
      "metadata": {
        "id": "Gkc9yuS0nQkZ"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_excel(data)"
      ],
      "metadata": {
        "id": "itT-oW4knScp"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scattertext"
      ],
      "metadata": {
        "id": "0o37kRxAi2NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scattertext as st\n",
        "import spacy\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "fTGGvyOOi2Po"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# from replacers import RegexpReplacer\n",
        "# 불용어 리스트를 불러옵니다.\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "gQDEE6p1i2Re"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scattertext.CorpusFromParsedDocuments import CorpusFromParsedDocuments\n",
        "from scattertext import SampleCorpora, whitespace_nlp_with_sentences, produce_scattertext_explorer"
      ],
      "metadata": {
        "id": "Cy6cGbzylqN3"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convention_df = data_df\n",
        "convention_df.iloc[0]"
      ],
      "metadata": {
        "id": "vvMzhJ7UjCm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "rgt9OH4hAidS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "# from replacers import RegexpReplacer\n",
        "# 불용어 리스트를 불러옵니다.\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "PXPaMRcbAifa"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "paragraph = convention_df.lower()\n",
        "print(paragraph)"
      ],
      "metadata": {
        "id": "Gp7LopjCAihw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "data_df['clean_doc'] = data_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "data_df['clean_doc'] = data_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "data_df['clean_doc'] = data_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "2IptIlePJbPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
        "stop_words.extend(['result', 'use', 'also', 'analyze', 'use', 'study', 'research','analysis','paper','purpose','using','first','toward','directly',\n",
        "                   'among','purpose','results'])\n",
        "convention_df['clean_doc'] = convention_df['clean_doc'].apply(lambda x: x.split()) # 토큰화"
      ],
      "metadata": {
        "id": "Uxptt5inI858"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
        "stop_words.extend(['result', 'use', 'also', 'analyze', 'use', 'study', 'research','analysis','paper','purpose','using','first','toward','directly',\n",
        "                   'among','purpose','results', 'glows', 'bleding', '2005', 'research', 'both', 'will', 'limitationsimplications', 'andor'])\n",
        "tokenized_doc = data_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "# 불용어를 제거합니다."
      ],
      "metadata": {
        "id": "GHydX2vDJwXk"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "corpus = st.CorpusFromPandas(convention_df,\n",
        "                             category_col='time',\n",
        "                             text_col='abs',\n",
        "                             nlp=nlp).build()"
      ],
      "metadata": {
        "id": "ixDXe1y2jCpM"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(corpus.get_scaled_f_scores_vs_background().index[:10]))"
      ],
      "metadata": {
        "id": "9Vf5nZlLjCsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_freq_df = corpus.get_term_freq_df()\n",
        "term_freq_df['Top 00-09 Score'] = corpus.get_scaled_f_scores('top 00-09')\n",
        "pprint(list(term_freq_df.sort_values(by='Top 00-09 Score', ascending=False).index[:10]))"
      ],
      "metadata": {
        "id": "DzBw3suqjCus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_freq_df = corpus.get_term_freq_df()\n",
        "term_freq_df['Top 10-19 Score'] = corpus.get_scaled_f_scores('top 10-19')\n",
        "pprint(list(term_freq_df.sort_values(by='Top 10-19 Score', ascending=False).index[:10]))"
      ],
      "metadata": {
        "id": "7Ou3mUhAjCzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = st.produce_scattertext_explorer(corpus,\n",
        "                                       category='top 00-09',\n",
        "                                       category_name='00-09',\n",
        "                                       not_category_name='10-19',\n",
        "                                       width_in_pixels=1000,\n",
        "                                       metadata=convention_df['time'])\n",
        "\n",
        "open(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))"
      ],
      "metadata": {
        "id": "h3HmC5GOjC1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7d139f-6b51-4af1-a142-ce8e501f9310"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2250150"
            ]
          },
          "metadata": {},
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZwDt6Y3Egvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OtjMDz3hEgyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSmKPeuPEg2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pz5KolGhEg3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}